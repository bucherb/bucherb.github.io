<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Bernadette Bucher</title>

  <meta name="author" content="Bernadette Bucher">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Bernadette Bucher</name>
              </p>
              <p>
                I am a PhD Student in the <a href="https://www.grasp.upenn.edu/">GRASP lab</a> at <a href="https://home.www.upenn.edu/">University of Pennsylvania</a> advised by
                <a href="http://www.cis.upenn.edu/~kostas/">Dr. Kostas Daniilidis</a>.
</p><p>
                My research interests focus on increasing sample efficiency in learning methods using visual sensory data
                to make autonomous decisions. In particular, my current work focuses on designing intrinsic rewards for exploration in reinforcement
                learning methods in order to increase sample efficiency via adaptive sampling.
</p><p>
                Prior to starting my PhD, I was a Senior Software Engineer at <a href="https://www.lockheedmartin.com/en-us/index.html">Lockheed Martin Corporation</a>
                where I worked from 2014 to 2019. I received an M.A. in Mathematics, M.A. in Economics, and
                B.S. in Mathematics and Economics from <a href="https://www.ua.edu/">The University of Alabama</a> in 2014.
              </p>
              <p style="text-align:center">
                <a href="mailto:bucherb@seas.upenn.edu">Email</a> &nbsp/&nbsp
                <a href="data/BernadetteBucher.pdf">CV</a> &nbsp/&nbsp
                <a href="data/Bucher-bio.txt">Biography</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=VIZvaGsAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/bernadette-bucher-09898536/"> LinkedIn </a>
              </p>
            </td>
            <td style="padding:2.5%;width:25%;max-width:25%">
              <a href="images/Bucher.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Bucher.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/adversarial.png" alt="fast-texture" width="160" height="120">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2003.06082">
                    <papertitle>Adversarial Curiosity</papertitle>
                  </a>
                  <br>
                  <strong>Bernadette Bucher*</strong>, <a href="https://sites.google.com/view/karlschmeckpeper">Karl Schmeckpeper*</a>, <a href="https://nikolaimatni.github.io/">Nikolai Matni</a>, <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>
                  <br>
                  <em>arXiv Preprint</em>, 2020
                  <br>
                  <a href="https://arxiv.org/abs/2003.06082">arXiv</a> &nbsp/&nbsp <a href="https://sites.google.com/view/action-for-better-prediction">project page</a> &nbsp/&nbsp <a href="https://github.com/bucherb/adversarial-curiosity">code</a> &nbsp/&nbsp <a href="data/Adversarial2020.bib">bibtex</a>
                  <p>We present a model-based adversarial curiosity method which addresses the scalability issues of prior model-based curiosity approaches through the use of a discriminator network.
                      Our method improves performance in multiple prediction-planning pipelines over baseline exploration strategies.
                  </p>
                </td>
              </tr>
            <tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/domain.png" alt="fast-texture" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://rss2020vlrrm.github.io/papers/10_CameraReadySubmission_ActionForBetterPrediction_CameraReady.pdf">
                    <papertitle>Action for Better Prediction</papertitle>
                  </a>
                  <br>
                  <strong>Bernadette Bucher*</strong>, <a href="https://sites.google.com/view/karlschmeckpeper">Karl Schmeckpeper*</a>, <a href="https://nikolaimatni.github.io/">Nikolai Matni</a>, <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>
                  <br>
                  <em>RSS Workshop on Visual Learning and Reasoning for Robotic Manipulation</em>, 2020
                  <br>
                  <a href="https://sites.google.com/view/action-for-better-prediction">project page</a> &nbsp/&nbsp <a href="data/Action2020.bib">bibtex</a>
                  <p>We show that a targeted sampling approach based on minimizing our novel curiosity-driven objective, leads to sample efficient prediction performance improvements in a domain transfer problem.
                  </p>
                </td>
              </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/multiagents.png" alt="fast-texture" width="160" height="120">
              </td>
              <td width="75%" valign="middle">
                <a href="https://baicsworkshop.github.io/pdf/BAICS_12.pdf">
                  <papertitle>Curiosity Increases Equality in Competitive Resource Allocation</papertitle>
                </a>
                <br>
                <strong>Bernadette Bucher*</strong>, <a href="https://www.seas.upenn.edu/~sidsingh/">Siddharth Singh*</a>, <a href="http://lptms.u-psud.fr/clelia-de-mulatier/">Cl&eacute;lia de Mulatier</a>, <a href="https://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>, <a href="https://www.sas.upenn.edu/~vbalasub/public-html/Home.html">Vijay Balasubramanian</a>
                <br>
                <em>ICLR Workshop on Bridging AI and Cognitive Science</em>, 2020
                <br>
                <a href="data/ICLR2020.bib">bibtex</a>
                <p> We show that giving agents a count-based reward for curiosity in a competitive resource allocation problem changes the system dynamics and ultimately increases equality of the resource distribution at equilibrium.
                </p>
              </td>
            </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/robonet.gif" alt="fast-texture" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/1910.11215.pdf">
                <papertitle>RoboNet: Large-Scale Multi-Robot Learning</papertitle>
              </a>
              <br>
              <a href="https://sudeepdasari.github.io/">Sudeep Dasari</a>, <a href="https://febert.github.io/">Frederik Ebert</a>, <a href="https://s-tian.github.io/">Stephen Tian</a>, <a href="https://cs.stanford.edu/~surajn/">Suraj Nair</a>, <strong>Bernadette Bucher</strong>, <a href="https://sites.google.com/view/karlschmeckpeper">Karl Schmeckpeper</a>, <a href="https://www.seas.upenn.edu/~sidsingh/">Siddharth Singh</a>, <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>, <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>
              <br>
              <em>Conference on Robot Learning (CoRL)</em>, 2019
              <br>
              <em>NeurIPS Workshop on Deep Reinforcement Learning</em>, 2019
              <br>
              <a href="https://arxiv.org/abs/1910.11215">arXiv</a> &nbsp/&nbsp <a href="https://www.robonet.wiki/">project page</a> &nbsp/&nbsp <a href="https://github.com/SudeepDasari/RoboNet">code</a> &nbsp/&nbsp <a href="data/CORL2019.bib">bibtex</a>
              <p>We developed a dataset of over 15 million video frames of 7 different robots at 113 different camera viewpoints interacting with objects. We use our new dataset to test the generalization capability of state-of-the-art video prediction algorithms.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/privateEye.png" alt="fast-texture" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://drive.google.com/file/d/1ec2d1URU4oVS4xM2QZzpYy5jQ67zpenX/view">
                <papertitle>Perception-Driven Curiosity with Bayesian Surprise</papertitle>
              </a>
              <br>
              <strong>Bernadette Bucher</strong>, Anton Arapin, Ramanan Sekar, Feifei Duan, <a href="https://www.ocf.berkeley.edu/~badger/">Marc Badger</a>, <a href="http://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>, <a href="https://www.seas.upenn.edu/~oleh/">Oleh Rybkin</a>
              <br>
              <em>RSS Workshop on Combining Learning and Reasoning Towards Human-Level Robot Intelligence</em>, 2019
              <br>
              <a href="data/RSS2019.bib">bibtex</a>
              <p> We model scene dynamics with a conditional variational autoencoder from which we compute an intrinsic reward for curiosity for use in a reinforcement learning algorithm.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/monoDepth.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://scene-understanding.com/papers/UnsupervisedMonocularDepthAndLatentStructure.pdf.pdf">
                <papertitle>Unsupervised Monocular Depth And Latent Structure</papertitle>
              </a>
              <br>
              Kenneth Chaney*, <strong>Bernadette Bucher*</strong>, Evangelos Chatzipantazis, <a href="http://www.cis.upenn.edu/~jshi/">Jianbo Shi</a>, <a href="http://www.cis.upenn.edu/~kostas/">Kostas Daniilidis</a>
              <br>
              <em>CVPR Workshop on 3D Scene Understanding for Vision, Graphics, and Robotics</em>, 2019
              <br>
              <a href="data/CVPR2019.bib">bibtex</a>
              <p> We demonstrate a novel method for learning distinct latent representations of structural and semantic information from single monocular images which we use for novel viewpoint synthesis.
              </p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">An inspirational website.</a>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
